{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_LSTM_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVC0k70TZWzf"
      },
      "source": [
        "This is just an example of how to use an LSTM in a network module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrHbnFJoZSoG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv1d, Dropout, Linear, LSTM, ELU, ReLU, GroupNorm\n",
        "# more info about these different layers can be found here https://pytorch.org/docs/stable/nn.html \n",
        " \n",
        "class zModel(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(zModel,self).__init__()\n",
        "        # the first two numbers in each constructor correspond to the input and output channels of your data respectively \n",
        "        self.conv1=Conv1d(3,16,kernel_size=1, stride=1)\n",
        "        self.conv2=Conv1d(16,16,kernel_size=2,stride=2)\n",
        "        self.conv3=Conv1d(16,16,kernel_size=3,stride=2)\n",
        "        self.fc1=Linear(10,32)\n",
        "        self.fc2=Linear(5,32)\n",
        "        self.fc3=Linear(2,32)\n",
        "        self.fc4=Linear(32,128)\n",
        "        self.fc5=Linear(128,64)\n",
        "        self.fc6=Linear(64,32)\n",
        "        self.fc7=Linear(32,1)\n",
        "        # the first two numbers in this constructor are input channel size, hidden layer size, number of reccurrent layers \n",
        "        self.lstm1=LSTM(32,16,32)\n",
        "        # the next two are dummy variables to keep track of the hidden states of the LSTM\n",
        "        self.h1=torch.rand(32,1,16).to(device)\n",
        "        self.c1=torch.rand(32,1,16).to(device)\n",
        "        # the .1 in here stands for 10% dropout rate \n",
        "        self.drop=Dropout(.1)\n",
        "        self.elu=ELU()\n",
        "        self.relu=ReLU()\n",
        "        # the arguments in this constructor correspond to the number of groups you want the layer to output, and the number of channels in the groups \n",
        "        self.laynorm=GroupNorm(1,32)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.drop(x)\n",
        "        res1=self.fc1(x[:,-1:,:])\n",
        "\n",
        "        x=self.conv2(x)\n",
        "        x=self.drop(x)\n",
        "        res2=self.fc2(x[:,-1:,:])\n",
        "\n",
        "        x=self.conv3(x)\n",
        "        x=self.drop(x)\n",
        "        res3=self.fc3(x[:,-1:,:])\n",
        "\n",
        "        x=self.drop(self.relu(self.fc4(x.view(1,1,-1))))\n",
        "        x=self.drop(self.relu(self.fc5(x)))\n",
        "        x=self.fc6(x)\n",
        "\n",
        "        x=self.laynorm(self.elu(x+res1+res2+res3).view(1,32,1))\n",
        "        x=self.drop(x)\n",
        "\n",
        "        self.h1 = self.h1.detach()\n",
        "        self.c1 = self.c1.detach()\n",
        "        out,(h,c)=self.lstm1(x.view(1,1,-1),(self.h1,self.c1))\n",
        "        self.h1 = h\n",
        "        self.c1 = c\n",
        "\n",
        "        out=self.fc7(x.view(1,1,-1))\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}